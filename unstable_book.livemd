# Unstable Book

```elixir
Mix.install([
  {:axon, github: "elixir-nx/axon"},
  {:exla, "~> 0.2.2"},
  {:nx, "~> 0.3.0", github: "elixir-nx/nx", sparse: "nx", branch: "main", override: true},
  {:req, "~> 0.3.0"},
  {:html_entities, "~> 0.5"}
])

EXLA.set_as_nx_default([:tpu, :cuda, :rocm, :host])
```

## Intro

An attempt to write a working implementation of Stable Diffusion inference in Elixir with Axon

## Prompt Tokenizer

SimpleTokenizer code copied from the [Keras implementation](https://github.com/keras-team/keras-cv/blob/036eda9d039226101a7170db81a0bc3dbbff8d85/keras_cv/models/generative/stable_diffusion/clip_tokenizer.py#L153)

```elixir
defmodule SimpleTokenizer do
  use GenServer

  @pat ~r/<\|startoftext\|>|<\|endoftext\|>|'s|'t|'re|'ve|'m|'ll|'d|\p{L}+|\p{N}|[^\s\p{L}\p{N}]+/iu

  @bpe_vocab_list "https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true"

  def start_link(_) do
    GenServer.start_link(__MODULE__, [], name: __MODULE__)
  end

  def vocab() do
    GenServer.call(__MODULE__, {:vocab})
  end

  def encode(text) do
    GenServer.call(__MODULE__, {:encode, text})
  end

  def decode(encoding) do
    GenServer.call(__MODULE__, {:decode, encoding})
  end

  @impl true
  def init(_) do
    vocab = build_vocab_list()
    {:ok, vocab}
  end

  @impl true
  def handle_call({:encode, text}, _from, vocab) do
    encoding = _encode(text, vocab)
    {:reply, encoding, vocab}
  end

  @impl true
  def handle_call({:decode, encoding}, _from, vocab) do
    reversed = _decode(encoding, vocab)
    {:reply, reversed, vocab}
  end

  @impl true
  def handle_call({:vocab}, _from, vocab) do
    {:reply, vocab, vocab}
  end

  def _encode(text, vocab) do
    Regex.scan(@pat, clean(text))
    |> List.flatten()
    |> Enum.map(fn token ->
      token
      |> :binary.bin_to_list()
      |> Enum.map(&Map.get(vocab[:byte_encoder], &1))
      |> Enum.join()
    end)
    |> Enum.map(&bpe(&1, vocab))
    |> List.flatten()
    |> then(fn words ->
      [Map.get(vocab[:encoder], "<|startoftext|>")] ++
        Enum.map(words, &Map.get(vocab[:encoder], &1)) ++
        [Map.get(vocab[:encoder], "<|endoftext|>")]
    end)
  end

  def _decode(encoding, vocab) do
    {encoding, vocab}
  end

  def clean(text) do
    text
    |> HtmlEntities.decode()
    |> HtmlEntities.decode()
    |> String.replace(~r/\s+/, " ")
    |> String.downcase()
  end

  def bytes_to_unicode() do
    codepoints =
      [?!..?~, ?¡..?¬, ?®..?ÿ]
      |> Enum.map(&Enum.to_list/1)
      |> Enum.concat()

    extras = for c <- 0..255, !Enum.member?(codepoints, c), do: c

    Map.new(
      Enum.zip(
        Enum.concat(codepoints, extras),
        Enum.map(
          Enum.concat(codepoints, Enum.with_index(extras, fn _c, index -> 256 + index end)),
          &to_string([&1])
        )
      )
    )
  end

  def fetch_vocab_list() do
    :inets.start()

    {:ok, {_status, _headers, body}} = :httpc.request([@bpe_vocab_list])

    body
    |> :zlib.gunzip()
    |> String.split("\n")
    |> Enum.slice(1, 49152 - 256 - 2)
    |> Enum.map(&String.split(&1))
    |> Enum.map(&List.to_tuple/1)
  end

  def build_vocab_list() do
    byte_encoder = bytes_to_unicode()
    bpe_vocab_subset = fetch_vocab_list()

    vocab =
      Enum.concat([
        Map.values(byte_encoder),
        Enum.map(Map.values(byte_encoder), &(&1 <> "</w>")),
        Enum.map(bpe_vocab_subset, &Enum.join(Tuple.to_list(&1))),
        ["<|startoftext|>", "<|endoftext|>"]
      ])

    encoder = Map.new(Enum.zip(vocab, 0..length(vocab)))

    %{
      byte_encoder: byte_encoder,
      encoder: encoder,
      decoder: Map.new(encoder, fn {key, val} -> {val, key} end),
      bpe_ranks: Map.new(Enum.zip(bpe_vocab_subset, 0..length(bpe_vocab_subset)))
    }
  end

  # A word is represented as a tuple of symbols (symbols being variable-length strings).
  def get_word(token) do
    token
    |> String.graphemes()
    |> Enum.reverse()
    |> then(fn [last | rest] -> [last <> "</w>" | rest] end)
    |> Enum.reverse()
  end

  def get_pairs(word) do
    word
    |> Enum.chunk_every(2, 1, :discard)
    |> Enum.map(&List.to_tuple/1)
  end

  def pairs_set(word), do: MapSet.new(get_pairs(word))

  def merge_word(selected, [a | [b | rest]], acc) when {a, b} == selected do
    merge_word(selected, rest, [a <> b | acc])
  end

  def merge_word(selected, [a | [b | rest]], acc) do
    merge_word(selected, [b | rest], [a | acc])
  end

  def merge_word(selected, [a | []], acc) do
    merge_word(selected, [], [a | acc])
  end

  def merge_word(_selected, [], acc), do: Enum.reverse(acc)

  def flatten_words(word, _, _) when length(word) == 1, do: word

  def flatten_words(word, max_rank, vocab) do
    set = pairs_set(word)
    best_ranked = Enum.min_by(set, fn pair -> Map.get(vocab[:bpe_ranks], pair, max_rank) end)
    rank = Map.get(vocab[:bpe_ranks], best_ranked, max_rank)

    cond do
      rank == max_rank -> word
      true -> flatten_words(merge_word(best_ranked, word, []), max_rank, vocab)
    end
  end

  def bpe(token, vocab) do
    word = get_word(token)
    max_rank = Enum.count(vocab[:bpe_ranks])
    flatten_words(word, max_rank, vocab)
  end
end
```

```elixir
vocab = SimpleTokenizer.build_vocab_list()
Enum.count(Map.get(vocab, :byte_encoder))
```

```elixir
{:ok, _} = SimpleTokenizer.start_link([])
SimpleTokenizer.encode("an astonaut riding a horse on the moon")
```
